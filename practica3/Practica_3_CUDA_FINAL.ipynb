{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica_3_CUDA_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hnsaavedraa/Computacion_Paralela/blob/master/practica3/Practica_3_CUDA_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgQiXkIPPGh7",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "19a1b7de-8611-40f1-cec2-e6996bfdcf1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title cuda Install\n",
        "!echo CUDA AND NVIDIA INSTALLATION\n",
        "!echo NOTE: THERE IS A QUESTION IN THE INSTALLATION, PLEASE DO NOT FORGET TO ANSWER IT (YOU CAN CHOOSE Y)\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb;\n",
        "!dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb;\n",
        "!apt-get update -qq;\n",
        "!apt-get install cuda-8.0;\n",
        "!ln -sf /usr/local/cuda-8.0 /usr/local/cuda\n",
        "\n",
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/cuda/lib'\n",
        "\n",
        "!apt-get install gcc-5 g++-5 -y -qq;\n",
        "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
        "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA AND NVIDIA INSTALLATION\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 0: `echo NOTE: THERE IS A QUESTION IN THE INSTALLATION, PLEASE DO NOT FORGET TO ANSWER IT (YOU CAN CHOOSE Y)'\n",
            "--2020-05-17 17:31:33--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2690 (2.6K) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604_8.0.61-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   2.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-17 17:31:33 (158 MB/s) - ‘cuda-repo-ubuntu1604_8.0.61-1_amd64.deb’ saved [2690/2690]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1604 (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604 (8.0.61-1) ...\n",
            "\n",
            "Configuration file '/etc/apt/sources.list.d/cuda.list'\n",
            " ==> File on system created by you or by a script.\n",
            " ==> File also in package provided by package maintainer.\n",
            "   What would you like to do about it ?  Your options are:\n",
            "    Y or I  : install the package maintainer's version\n",
            "    N or O  : keep your currently-installed version\n",
            "      D     : show the differences between the versions\n",
            "      Z     : start a shell to examine the situation\n",
            " The default action is to keep your current version.\n",
            "*** cuda.list (Y/I/N/O/D/Z) [default=N] ? n\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604)\n",
            "OK\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-8.0-1' for regex 'cuda-8.0'\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Selecting previously unselected package gcc-5-base:amd64.\n",
            "(Reading database ... 144436 files and directories currently installed.)\n",
            "Preparing to unpack .../0-gcc-5-base_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking gcc-5-base:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libisl15:amd64.\n",
            "Preparing to unpack .../1-libisl15_0.18-4_amd64.deb ...\n",
            "Unpacking libisl15:amd64 (0.18-4) ...\n",
            "Selecting previously unselected package cpp-5.\n",
            "Preparing to unpack .../2-cpp-5_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking cpp-5 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libasan2:amd64.\n",
            "Preparing to unpack .../3-libasan2_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libasan2:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libmpx0:amd64.\n",
            "Preparing to unpack .../4-libmpx0_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libmpx0:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libgcc-5-dev:amd64.\n",
            "Preparing to unpack .../5-libgcc-5-dev_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package gcc-5.\n",
            "Preparing to unpack .../6-gcc-5_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking gcc-5 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package libstdc++-5-dev:amd64.\n",
            "Preparing to unpack .../7-libstdc++-5-dev_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Selecting previously unselected package g++-5.\n",
            "Preparing to unpack .../8-g++-5_5.5.0-12ubuntu1_amd64.deb ...\n",
            "Unpacking g++-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up libisl15:amd64 (0.18-4) ...\n",
            "Setting up gcc-5-base:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up libmpx0:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up libasan2:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up cpp-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...\n",
            "Setting up gcc-5 (5.5.0-12ubuntu1) ...\n",
            "Setting up g++-5 (5.5.0-12ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-kofmhdse\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-kofmhdse\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=84c85250ecad74a62b54a306afbd1ce44343bd94be0959db0a12eab96763f67f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cgwkl4k3/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnim4yMkqzXI",
        "colab_type": "code",
        "outputId": "dd35e00b-5700-4fa4-bac7-e61f2288d27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/720.jpg\n",
        "!wget https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/1080.jpg\n",
        "!wget https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/4k.jpg\n",
        "!wget https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/stb_image/stb_image.h\n",
        "!wget https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/stb_image/stb_image_write.h\n",
        "!wget https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/stb_image/stb_image_resize.h\n",
        "\n",
        "!mkdir src\n",
        "!mkdir src/stb_image\n",
        "!mv *.jpg src/\n",
        "!mv *.h src/stb_image/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-17 17:33:15--  https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/720.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 465450 (455K) [image/jpeg]\n",
            "Saving to: ‘720.jpg’\n",
            "\n",
            "\r720.jpg               0%[                    ]       0  --.-KB/s               \r720.jpg             100%[===================>] 454.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-05-17 17:33:16 (18.1 MB/s) - ‘720.jpg’ saved [465450/465450]\n",
            "\n",
            "--2020-05-17 17:33:18--  https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/1080.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 359455 (351K) [image/jpeg]\n",
            "Saving to: ‘1080.jpg’\n",
            "\n",
            "1080.jpg            100%[===================>] 351.03K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-05-17 17:33:19 (16.4 MB/s) - ‘1080.jpg’ saved [359455/359455]\n",
            "\n",
            "--2020-05-17 17:33:20--  https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/4k.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 493340 (482K) [image/jpeg]\n",
            "Saving to: ‘4k.jpg’\n",
            "\n",
            "4k.jpg              100%[===================>] 481.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-05-17 17:33:21 (19.3 MB/s) - ‘4k.jpg’ saved [493340/493340]\n",
            "\n",
            "--2020-05-17 17:33:22--  https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/stb_image/stb_image.h\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 267319 (261K) [text/plain]\n",
            "Saving to: ‘stb_image.h’\n",
            "\n",
            "stb_image.h         100%[===================>] 261.05K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-05-17 17:33:23 (14.4 MB/s) - ‘stb_image.h’ saved [267319/267319]\n",
            "\n",
            "--2020-05-17 17:33:24--  https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/stb_image/stb_image_write.h\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68990 (67K) [text/plain]\n",
            "Saving to: ‘stb_image_write.h’\n",
            "\n",
            "stb_image_write.h   100%[===================>]  67.37K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-05-17 17:33:25 (8.47 MB/s) - ‘stb_image_write.h’ saved [68990/68990]\n",
            "\n",
            "--2020-05-17 17:33:27--  https://raw.githubusercontent.com/hnsaavedraa/Computacion_Paralela/master/practica1/stb_image/stb_image_resize.h\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116073 (113K) [text/plain]\n",
            "Saving to: ‘stb_image_resize.h’\n",
            "\n",
            "stb_image_resize.h  100%[===================>] 113.35K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-17 17:33:27 (9.80 MB/s) - ‘stb_image_resize.h’ saved [116073/116073]\n",
            "\n",
            "mkdir: cannot create directory ‘src’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qRJcQeYLfrf",
        "colab_type": "code",
        "outputId": "274c1ebc-55ca-4461-e81a-c4916ff9bde6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd  /usr/local/cuda/samples/1_Utilities/deviceQuery/\n",
        "!ls\n",
        "!make\n",
        "!./deviceQuery\n",
        "!nvcc --version\n",
        "%cat /usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery.cpp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n",
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.1\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15080 MBytes (15812263936 bytes)\n",
            "  (40) Multiprocessors, ( 64) CUDA Cores/MP:     2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1\n",
            "Result = PASS\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "/*\n",
            " * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.\n",
            " *\n",
            " * Please refer to the NVIDIA end user license agreement (EULA) associated\n",
            " * with this source code for terms and conditions that govern your use of\n",
            " * this software. Any use, reproduction, disclosure, or distribution of\n",
            " * this software and related documentation outside the terms of the EULA\n",
            " * is strictly prohibited.\n",
            " *\n",
            " */\n",
            "/* This sample queries the properties of the CUDA devices present in the system\n",
            " * via CUDA Runtime API. */\n",
            "\n",
            "// std::system includes\n",
            "\n",
            "#include <cuda_runtime.h>\n",
            "#include <helper_cuda.h>\n",
            "\n",
            "#include <iostream>\n",
            "#include <memory>\n",
            "#include <string>\n",
            "\n",
            "int *pArgc = NULL;\n",
            "char **pArgv = NULL;\n",
            "\n",
            "#if CUDART_VERSION < 5000\n",
            "\n",
            "// CUDA-C includes\n",
            "#include <cuda.h>\n",
            "\n",
            "// This function wraps the CUDA Driver API into a template function\n",
            "template <class T>\n",
            "inline void getCudaAttribute(T *attribute, CUdevice_attribute device_attribute,\n",
            "                             int device) {\n",
            "  CUresult error = cuDeviceGetAttribute(attribute, device_attribute, device);\n",
            "\n",
            "  if (CUDA_SUCCESS != error) {\n",
            "    fprintf(\n",
            "        stderr,\n",
            "        \"cuSafeCallNoSync() Driver API error = %04d from file <%s>, line %i.\\n\",\n",
            "        error, __FILE__, __LINE__);\n",
            "\n",
            "    exit(EXIT_FAILURE);\n",
            "  }\n",
            "}\n",
            "\n",
            "#endif /* CUDART_VERSION < 5000 */\n",
            "\n",
            "////////////////////////////////////////////////////////////////////////////////\n",
            "// Program main\n",
            "////////////////////////////////////////////////////////////////////////////////\n",
            "int main(int argc, char **argv) {\n",
            "  pArgc = &argc;\n",
            "  pArgv = argv;\n",
            "\n",
            "  printf(\"%s Starting...\\n\\n\", argv[0]);\n",
            "  printf(\n",
            "      \" CUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n",
            "\n",
            "  int deviceCount = 0;\n",
            "  cudaError_t error_id = cudaGetDeviceCount(&deviceCount);\n",
            "\n",
            "  if (error_id != cudaSuccess) {\n",
            "    printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\",\n",
            "           static_cast<int>(error_id), cudaGetErrorString(error_id));\n",
            "    printf(\"Result = FAIL\\n\");\n",
            "    exit(EXIT_FAILURE);\n",
            "  }\n",
            "\n",
            "  // This function call returns 0 if there are no CUDA capable devices.\n",
            "  if (deviceCount == 0) {\n",
            "    printf(\"There are no available device(s) that support CUDA\\n\");\n",
            "  } else {\n",
            "    printf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
            "  }\n",
            "\n",
            "  int dev, driverVersion = 0, runtimeVersion = 0;\n",
            "\n",
            "  for (dev = 0; dev < deviceCount; ++dev) {\n",
            "    cudaSetDevice(dev);\n",
            "    cudaDeviceProp deviceProp;\n",
            "    cudaGetDeviceProperties(&deviceProp, dev);\n",
            "\n",
            "    printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
            "\n",
            "    // Console log\n",
            "    cudaDriverGetVersion(&driverVersion);\n",
            "    cudaRuntimeGetVersion(&runtimeVersion);\n",
            "    printf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
            "           driverVersion / 1000, (driverVersion % 100) / 10,\n",
            "           runtimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
            "    printf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
            "           deviceProp.major, deviceProp.minor);\n",
            "\n",
            "    char msg[256];\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "    sprintf_s(msg, sizeof(msg),\n",
            "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
            "             \"(%llu bytes)\\n\",\n",
            "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
            "             (unsigned long long)deviceProp.totalGlobalMem);\n",
            "#else\n",
            "    snprintf(msg, sizeof(msg),\n",
            "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
            "             \"(%llu bytes)\\n\",\n",
            "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
            "             (unsigned long long)deviceProp.totalGlobalMem);\n",
            "#endif\n",
            "    printf(\"%s\", msg);\n",
            "\n",
            "    printf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n",
            "           deviceProp.multiProcessorCount,\n",
            "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n",
            "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n",
            "               deviceProp.multiProcessorCount);\n",
            "    printf(\n",
            "        \"  GPU Max Clock rate:                            %.0f MHz (%0.2f \"\n",
            "        \"GHz)\\n\",\n",
            "        deviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n",
            "\n",
            "#if CUDART_VERSION >= 5000\n",
            "    // This is supported in CUDA 5.0 (runtime API device properties)\n",
            "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
            "           deviceProp.memoryClockRate * 1e-3f);\n",
            "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
            "           deviceProp.memoryBusWidth);\n",
            "\n",
            "    if (deviceProp.l2CacheSize) {\n",
            "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
            "             deviceProp.l2CacheSize);\n",
            "    }\n",
            "\n",
            "#else\n",
            "    // This only available in CUDA 4.0-4.2 (but these were only exposed in the\n",
            "    // CUDA Driver API)\n",
            "    int memoryClock;\n",
            "    getCudaAttribute<int>(&memoryClock, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE,\n",
            "                          dev);\n",
            "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
            "           memoryClock * 1e-3f);\n",
            "    int memBusWidth;\n",
            "    getCudaAttribute<int>(&memBusWidth,\n",
            "                          CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, dev);\n",
            "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
            "           memBusWidth);\n",
            "    int L2CacheSize;\n",
            "    getCudaAttribute<int>(&L2CacheSize, CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE, dev);\n",
            "\n",
            "    if (L2CacheSize) {\n",
            "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
            "             L2CacheSize);\n",
            "    }\n",
            "\n",
            "#endif\n",
            "\n",
            "    printf(\n",
            "        \"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, \"\n",
            "        \"%d), 3D=(%d, %d, %d)\\n\",\n",
            "        deviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n",
            "        deviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n",
            "        deviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n",
            "    printf(\n",
            "        \"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n",
            "        deviceProp.maxTexture1DLayered[0], deviceProp.maxTexture1DLayered[1]);\n",
            "    printf(\n",
            "        \"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d \"\n",
            "        \"layers\\n\",\n",
            "        deviceProp.maxTexture2DLayered[0], deviceProp.maxTexture2DLayered[1],\n",
            "        deviceProp.maxTexture2DLayered[2]);\n",
            "\n",
            "    printf(\"  Total amount of constant memory:               %lu bytes\\n\",\n",
            "           deviceProp.totalConstMem);\n",
            "    printf(\"  Total amount of shared memory per block:       %lu bytes\\n\",\n",
            "           deviceProp.sharedMemPerBlock);\n",
            "    printf(\"  Total number of registers available per block: %d\\n\",\n",
            "           deviceProp.regsPerBlock);\n",
            "    printf(\"  Warp size:                                     %d\\n\",\n",
            "           deviceProp.warpSize);\n",
            "    printf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n",
            "           deviceProp.maxThreadsPerMultiProcessor);\n",
            "    printf(\"  Maximum number of threads per block:           %d\\n\",\n",
            "           deviceProp.maxThreadsPerBlock);\n",
            "    printf(\"  Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n",
            "           deviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n",
            "           deviceProp.maxThreadsDim[2]);\n",
            "    printf(\"  Max dimension size of a grid size    (x,y,z): (%d, %d, %d)\\n\",\n",
            "           deviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n",
            "           deviceProp.maxGridSize[2]);\n",
            "    printf(\"  Maximum memory pitch:                          %lu bytes\\n\",\n",
            "           deviceProp.memPitch);\n",
            "    printf(\"  Texture alignment:                             %lu bytes\\n\",\n",
            "           deviceProp.textureAlignment);\n",
            "    printf(\n",
            "        \"  Concurrent copy and kernel execution:          %s with %d copy \"\n",
            "        \"engine(s)\\n\",\n",
            "        (deviceProp.deviceOverlap ? \"Yes\" : \"No\"), deviceProp.asyncEngineCount);\n",
            "    printf(\"  Run time limit on kernels:                     %s\\n\",\n",
            "           deviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
            "    printf(\"  Integrated GPU sharing Host Memory:            %s\\n\",\n",
            "           deviceProp.integrated ? \"Yes\" : \"No\");\n",
            "    printf(\"  Support host page-locked memory mapping:       %s\\n\",\n",
            "           deviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
            "    printf(\"  Alignment requirement for Surfaces:            %s\\n\",\n",
            "           deviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device has ECC support:                        %s\\n\",\n",
            "           deviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "    printf(\"  CUDA Device Driver Mode (TCC or WDDM):         %s\\n\",\n",
            "           deviceProp.tccDriver ? \"TCC (Tesla Compute Cluster Driver)\"\n",
            "                                : \"WDDM (Windows Display Driver Model)\");\n",
            "#endif\n",
            "    printf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n",
            "           deviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device supports Compute Preemption:            %s\\n\",\n",
            "           deviceProp.computePreemptionSupported ? \"Yes\" : \"No\");\n",
            "    printf(\"  Supports Cooperative Kernel Launch:            %s\\n\",\n",
            "           deviceProp.cooperativeLaunch ? \"Yes\" : \"No\");\n",
            "    printf(\"  Supports MultiDevice Co-op Kernel Launch:      %s\\n\",\n",
            "           deviceProp.cooperativeMultiDeviceLaunch ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n",
            "           deviceProp.pciDomainID, deviceProp.pciBusID, deviceProp.pciDeviceID);\n",
            "\n",
            "    const char *sComputeMode[] = {\n",
            "        \"Default (multiple host threads can use ::cudaSetDevice() with device \"\n",
            "        \"simultaneously)\",\n",
            "        \"Exclusive (only one host thread in one process is able to use \"\n",
            "        \"::cudaSetDevice() with this device)\",\n",
            "        \"Prohibited (no host thread can use ::cudaSetDevice() with this \"\n",
            "        \"device)\",\n",
            "        \"Exclusive Process (many threads in one process is able to use \"\n",
            "        \"::cudaSetDevice() with this device)\",\n",
            "        \"Unknown\",\n",
            "        NULL};\n",
            "    printf(\"  Compute Mode:\\n\");\n",
            "    printf(\"     < %s >\\n\", sComputeMode[deviceProp.computeMode]);\n",
            "  }\n",
            "\n",
            "  // If there are 2 or more GPUs, query to determine whether RDMA is supported\n",
            "  if (deviceCount >= 2) {\n",
            "    cudaDeviceProp prop[64];\n",
            "    int gpuid[64];  // we want to find the first two GPUs that can support P2P\n",
            "    int gpu_p2p_count = 0;\n",
            "\n",
            "    for (int i = 0; i < deviceCount; i++) {\n",
            "      checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));\n",
            "\n",
            "      // Only boards based on Fermi or later can support P2P\n",
            "      if ((prop[i].major >= 2)\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "          // on Windows (64-bit), the Tesla Compute Cluster driver for windows\n",
            "          // must be enabled to support this\n",
            "          && prop[i].tccDriver\n",
            "#endif\n",
            "      ) {\n",
            "        // This is an array of P2P capable GPUs\n",
            "        gpuid[gpu_p2p_count++] = i;\n",
            "      }\n",
            "    }\n",
            "\n",
            "    // Show all the combinations of support P2P GPUs\n",
            "    int can_access_peer;\n",
            "\n",
            "    if (gpu_p2p_count >= 2) {\n",
            "      for (int i = 0; i < gpu_p2p_count; i++) {\n",
            "        for (int j = 0; j < gpu_p2p_count; j++) {\n",
            "          if (gpuid[i] == gpuid[j]) {\n",
            "            continue;\n",
            "          }\n",
            "          checkCudaErrors(\n",
            "              cudaDeviceCanAccessPeer(&can_access_peer, gpuid[i], gpuid[j]));\n",
            "          printf(\"> Peer access from %s (GPU%d) -> %s (GPU%d) : %s\\n\",\n",
            "                 prop[gpuid[i]].name, gpuid[i], prop[gpuid[j]].name, gpuid[j],\n",
            "                 can_access_peer ? \"Yes\" : \"No\");\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "\n",
            "  // csv masterlog info\n",
            "  // *****************************\n",
            "  // exe and CUDA driver name\n",
            "  printf(\"\\n\");\n",
            "  std::string sProfileString = \"deviceQuery, CUDA Driver = CUDART\";\n",
            "  char cTemp[16];\n",
            "\n",
            "  // driver version\n",
            "  sProfileString += \", CUDA Driver Version = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d.%d\", driverVersion/1000, (driverVersion%100)/10);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", driverVersion / 1000,\n",
            "           (driverVersion % 100) / 10);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "\n",
            "  // Runtime version\n",
            "  sProfileString += \", CUDA Runtime Version = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d.%d\", runtimeVersion/1000, (runtimeVersion%100)/10);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", runtimeVersion / 1000,\n",
            "           (runtimeVersion % 100) / 10);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "\n",
            "  // Device count\n",
            "  sProfileString += \", NumDevs = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d\", deviceCount);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d\", deviceCount);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "  sProfileString += \"\\n\";\n",
            "  printf(\"%s\", sProfileString.c_str());\n",
            "\n",
            "  printf(\"Result = PASS\\n\");\n",
            "\n",
            "  // finish\n",
            "  exit(EXIT_SUCCESS);\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjaJPhgCLmDk",
        "colab_type": "code",
        "outputId": "a8f186ed-d0fc-46c6-986a-bcb4f9e6401d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNcZBhK4SLWX",
        "colab_type": "code",
        "outputId": "b62f43e5-ac3f-4c5a-d773-297a905a6ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name blur_effect.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <stdint.h>\n",
        "#include <string.h>\n",
        "#include <stdbool.h>\n",
        "#include <math.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#define STB_IMAGE_IMPLEMENTATION\n",
        "#define STB_IMAGE_WRITE_IMPLEMENTATION\n",
        "\n",
        "#include \"stb_image/stb_image.h\"\n",
        "#include \"stb_image/stb_image_write.h\"\n",
        "\n",
        "__device__ int myMax(int num1, int num2)\n",
        "{\n",
        "\treturn (num1 > num2) ? num1 : num2;\n",
        "}\n",
        "\n",
        "__device__ int myMin(int num1, int num2)\n",
        "{\n",
        "\treturn (num1 > num2) ? num2 : num1;\n",
        "}\n",
        "\n",
        "//Metodo que calcula el radio de las cajas (br) para el algoritmo boxblur\n",
        "void boxesForGauss(int sigma, int n, double *sizes) \n",
        "{\n",
        "\n",
        "\tdouble wIdeal = sqrt((12 * sigma * sigma / n) + 1); \n",
        "\tdouble wl = floor(wIdeal);\n",
        "\tif (fmod(wl, 2.0) == 0.0)\n",
        "\t{\n",
        "\t\twl--;\n",
        "\t}\n",
        "\tdouble wu = wl + 2;\n",
        "\n",
        "\tdouble mIdeal = (12 * sigma * sigma - n * wl * wl - 4 * n * wl - 3 * n) / (-4 * wl - 4);\n",
        "\tdouble m = round(mIdeal);\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t{\n",
        "\t\t*(sizes + i) = (i < m ? wl : wu);\n",
        "\t}\n",
        "}\n",
        "\n",
        "//Metodo ejecutado por los hilos\n",
        "__global__ void parallelProcess(int *scl, int *tcl, int w, int h, int r, int totalThreads)\n",
        "{\n",
        "\tint index = (blockDim.x * blockIdx.x) + threadIdx.x;\n",
        "\tint init_i = ((w*h)/totalThreads) * index;\n",
        "\tint end_i;\n",
        "\tif(index == totalThreads-1){\n",
        "\t\t\tend_i = ((w*h)- init_i);\n",
        "\t}\n",
        "\telse{\n",
        "\t\t\tend_i = ((w*h)/(totalThreads));\n",
        "\t}\n",
        "\tint i = init_i / w;\n",
        "\tint j = init_i % w;\n",
        "\tint control = 0;\n",
        "\n",
        "\twhile(control < end_i){\n",
        "\t\t\tint val = 0;\n",
        "\t\t\tfor (int ix = j - r; ix < j + r + 1; ix++)\n",
        "\t\t\t{\n",
        "\t\t\t\tint x = myMin(w - 1, myMax(0, ix));\n",
        "\t\t\t\tval += *(tcl + (i * w + x));\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\t*(scl + (i * w + j)) = val / (r + r + 1);\n",
        "\t\t\tj++;\n",
        "\t\t\tif(j == w){\n",
        "\t\t\t\t\tj = 0;\n",
        "\t\t\t\t\ti++;\n",
        "\t\t\t}\n",
        "\t\t\tif(i == h){\n",
        "\t\t\t\t\tbreak;\n",
        "\t\t\t}\n",
        "\t\t\tcontrol++;\n",
        "\t}\n",
        "\n",
        "\tint a = init_i / w;\n",
        "\tint b = init_i % w;\n",
        "\tint control_2 = 0;\n",
        "\n",
        "\twhile(control_2 < end_i){\n",
        "\t\t\tint val = 0;\n",
        "\t\t\tfor (int iy = a - r; iy < a + r + 1; iy++)\n",
        "\t\t\t{\n",
        "\t\t\t\tint y = myMin(h - 1, myMax(0, iy));\n",
        "\t\t\t\tval += *(scl + (y * w + b));\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\t*(tcl + (a * w + b)) = val / (r + r + 1);\n",
        "\t\t\tb++;\n",
        "\t\t\tif(b == w){\n",
        "\t\t\t\t\tb = 0;\n",
        "\t\t\t\t\ta++;\n",
        "\t\t\t}\n",
        "\t\t\tif(a == h){\n",
        "\t\t\t\t\tbreak;\n",
        "\t\t\t}\n",
        "\t\t\tcontrol_2++;\n",
        "\t}\n",
        "}\n",
        "\n",
        "//Metodo donde se crean los hilos y se asigna el trabajo de cada uno\n",
        "void boxBlur(int *scl, int *tcl, int w, int h, int r, int blocksPerGrid, int threadsPerBlock)\n",
        "{\n",
        "\tfor (int i = 0; i < (w * h); i++)\n",
        "\t{\n",
        "\t\tint aux = *(scl + i);\n",
        "\t\t*(tcl + i) = aux;\n",
        "\t}\n",
        "\n",
        "\tint *d_scl, *d_tcl;\n",
        "\tcudaError_t err = cudaSuccess;\n",
        "\n",
        "\terr = cudaMalloc((void **)&d_scl, sizeof(int)*w*h);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to allocate device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\n",
        "\terr = cudaMalloc((void **)&d_tcl, sizeof(int)*w*h);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to allocate device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\n",
        "\terr = cudaMemcpy(d_scl, scl, sizeof(int)*w*h, cudaMemcpyHostToDevice);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to copy vector C from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\n",
        "\terr = cudaMemcpy(d_tcl, tcl, sizeof(int)*w*h, cudaMemcpyHostToDevice);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to copy vector C from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\n",
        "\tint totalThreads;\n",
        "\ttotalThreads = blocksPerGrid * threadsPerBlock;\n",
        "\tparallelProcess<<<blocksPerGrid, threadsPerBlock>>>(d_scl, d_tcl, w, h, r, totalThreads);\n",
        "\n",
        "\terr = cudaMemcpy(scl, d_scl, sizeof(int)*w*h, cudaMemcpyDeviceToHost);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to copy vector from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\t\n",
        "\terr = cudaMemcpy(tcl, d_tcl, sizeof(int)*w*h, cudaMemcpyDeviceToHost);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to copy vector from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\n",
        "\terr = cudaFree(d_scl);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to free device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "\n",
        "\terr = cudaFree(d_tcl);\n",
        "\tif (err != cudaSuccess){\n",
        "\t\t\tfprintf(stderr, \"Failed to free device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "\t\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "}\n",
        "\n",
        "//Metodo que aplica las iteracion del algoritmo boxblur para aproximar Gaussian blur\n",
        "void gaussBlur_3(int *scl, int *tcl, int w, int h, int r, int blocksPerGrid, int threadsPerBlock)\n",
        "{\n",
        "\n",
        "\tdouble *bxs = (double *)malloc(sizeof(double) * 3);\n",
        "\tboxesForGauss(r, 3, bxs);\n",
        "\n",
        "\tboxBlur(scl, tcl, w, h, (int)((*(bxs)-1) / 2), blocksPerGrid, threadsPerBlock);\n",
        "\tboxBlur(tcl, scl, w, h, (int)((*(bxs + 1) - 1) / 2), blocksPerGrid, threadsPerBlock);\n",
        "\tboxBlur(scl, tcl, w, h, (int)((*(bxs + 2) - 1) / 2), blocksPerGrid, threadsPerBlock);\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\t// Cargamos la imagen y definimos variables \n",
        "\tint width, height, channels;\n",
        "\tchar img_name[128];\n",
        "\tstrcpy(img_name, argv[1]);\n",
        "\tchar new_img_name[128];\n",
        "\tchar *eptr;\n",
        "\tint kernel = atoi(argv[3]);\n",
        "\tdouble multiplicator = strtod(argv[4],&eptr);\n",
        "\tstrcpy(new_img_name, argv[2]);\n",
        "\tunsigned char *img = stbi_load(img_name, &width, &height, &channels, 0);\n",
        "\tcudaSetDevice(0);\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\tcudaGetDeviceProperties(&deviceProp, 0);\n",
        "\tint blocksPerGrid, threadsPerBlock, totalThreads;\n",
        "\tblocksPerGrid = deviceProp.multiProcessorCount;\t\n",
        "\tthreadsPerBlock = multiplicator *_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor);\n",
        "\ttotalThreads = blocksPerGrid * threadsPerBlock;\n",
        "\n",
        "\tprintf(\"CUDA kernel launch with %d blocks of %d threads Total: %i \\n\", blocksPerGrid, threadsPerBlock, totalThreads);\n",
        "\n",
        "\tif (img == NULL)\n",
        "\t{\n",
        "\t\tprintf(\"Error in loading the image\\n\");\n",
        "\t\texit(1);\n",
        "\t}\n",
        "\tprintf(\"Loaded image with a width of %dpx, a height of %dpx and %d channels\\n\", width, height, channels);\n",
        "\n",
        "\t// Almacenamos los valores de cada canal\n",
        "\tint img_size = width * height * channels;\n",
        "\tint *r = (int *)malloc(sizeof(int) * (img_size / 3));\n",
        "\tint *g = (int *)malloc(sizeof(int) * (img_size / 3));\n",
        "\tint *b = (int *)malloc(sizeof(int) * (img_size / 3));\n",
        "\tint i = 0;\n",
        "\tfor (unsigned char *p = img; p != img + img_size; p += channels, i++)\n",
        "\t{\n",
        "\t\t*(r + i) = (uint8_t)*p;\n",
        "\t\t*(g + i) = (uint8_t) * (p + 1);\n",
        "\t\t*(b + i) = (uint8_t) * (p + 2);\n",
        "\t}\n",
        "\n",
        "\t// Instanciamos los canales de la imagen de salida\n",
        "\tint *r_target = (int *)malloc(sizeof(int) * (img_size / 3));\n",
        "\tint *g_target = (int *)malloc(sizeof(int) * (img_size / 3));\n",
        "\tint *b_target = (int *)malloc(sizeof(int) * (img_size / 3));\n",
        "\tint j = 0;\n",
        "\n",
        "\t//Aplicamos el algoritmo para cada canal\n",
        "\tgaussBlur_3(r, r_target, width, height, kernel, blocksPerGrid, threadsPerBlock);\n",
        "\tgaussBlur_3(g, g_target, width, height, kernel, blocksPerGrid, threadsPerBlock);\n",
        "\tgaussBlur_3(b, b_target, width, height, kernel, blocksPerGrid, threadsPerBlock);\n",
        "\n",
        "\t// Se reconstruye la imagen a partir de los canales procesados\n",
        "\tfor (int i = 0; i < img_size / 3; i++)\n",
        "\t{\n",
        "\t\timg[j] = *(r_target + i);\n",
        "\t\timg[j + 1] = *(g_target + i);\n",
        "\t\timg[j + 2] = *(b_target + i);\n",
        "\t\tj += 3;\n",
        "\t}\n",
        "\n",
        "\t//Se crea la nueva imagen y liberamos memoria \n",
        "\tstbi_write_jpg(new_img_name, width, height, channels, img, 100);\n",
        "\tfree(r_target);\n",
        "\tfree(g_target);\n",
        "\tfree(b_target);\n",
        "\tfree(r);\n",
        "\tfree(g);\n",
        "\tfree(b);\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/blur_effect.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clKMnWbff3Xz",
        "colab_type": "code",
        "outputId": "297556a1-9cf6-42ed-d576-a48ffe216b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!nvcc -Xcompiler -Wall /content/src/blur_effect.cu -o /content/src/blur -I \"usr/local/cuda/samples/common/inc\" -lm "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/src/stb_image/stb_image.h(4096): warning: variable \"old_limit\" was set but never used\n",
            "\n",
            "/content/src/stb_image/stb_image.h(4975): warning: variable \"idata_limit_old\" was set but never used\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQnPinD2hKbt",
        "colab_type": "code",
        "outputId": "b82a6175-ed49-4d63-f997-20e38203026c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!time /content/src/blur /content/src/720.jpg /content/src/out.jpg 15 4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA kernel launch with 40 blocks of 256 threads Total: 10240 \n",
            "Loaded image with a width of 1280px, a height of 720px and 3 channels\n",
            "\n",
            "real\t0m0.553s\n",
            "user\t0m0.276s\n",
            "sys\t0m0.176s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_yXBE2gUxy",
        "colab_type": "code",
        "outputId": "db69d15e-e111-4b00-b96f-88ee80fd0ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "img_init=('/content/src/720.jpg' '/content/src/1080.jpg' '/content/src/4k.jpg')\n",
        "img_fin=('output720.jpg' 'output1080.jpg' 'output4k.jpg')\n",
        "touch times.txt\n",
        "FILE=/times.txt\n",
        "\n",
        "echo \"\" > times.txt\n",
        "\n",
        "\n",
        "for i in 0 1 2\n",
        "do\n",
        "\tfor kernel in 3 5 7 9 11 13 15\n",
        "\tdo\n",
        "\t\tfor NumHilos in 1\n",
        "\t\tdo\n",
        "\t\t\tfor intentos in 1\n",
        "\t\t\tdo\n",
        "\n",
        "\t\t\t\tmytime=\"$(time ( time /content/src/blur ${img_init[i]} ${img_fin[i]} $kernel $NumHilos ) 2>&1 1>/dev/null )\"\n",
        "\t\t\t\tjl=\"\"\n",
        "\t\t\t\n",
        "\t\t\t\techo \"Tiempo para imagen\" ${img_init[i]}  \" con un kernel de \" $kernel \" y con  \"  $NumHilos \" hilos , intento numero\" $intentos>> times.txt\n",
        "\t\t\t\techo $mytime >> times.txt\n",
        "\t\t\t\techo $jl >> times.txt\n",
        "\t\t\t\techo \"Tiempo para imagen\" ${img_init[i]}  \" con un kernel de \" $kernel \" y con  \"  $NumHilos \" hilos, intento numero\" $intentos\n",
        "\t\t\t\techo $mytime\n",
        "\n",
        "\t\t\tdone\n",
        "\n",
        "\t\t\t\n",
        "\t\tdone \n",
        "\tdone\n",
        "\t   \n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  3  y con   1  hilos, intento numero 1\n",
            "real 0m0.417s user 0m0.234s sys 0m0.168s\n",
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  5  y con   1  hilos, intento numero 1\n",
            "real 0m0.383s user 0m0.231s sys 0m0.140s\n",
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  7  y con   1  hilos, intento numero 1\n",
            "real 0m0.387s user 0m0.224s sys 0m0.143s\n",
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  9  y con   1  hilos, intento numero 1\n",
            "real 0m0.383s user 0m0.211s sys 0m0.155s\n",
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  11  y con   1  hilos, intento numero 1\n",
            "real 0m0.382s user 0m0.217s sys 0m0.151s\n",
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  13  y con   1  hilos, intento numero 1\n",
            "real 0m0.409s user 0m0.225s sys 0m0.166s\n",
            "Tiempo para imagen /content/src/720.jpg  con un kernel de  15  y con   1  hilos, intento numero 1\n",
            "real 0m0.413s user 0m0.241s sys 0m0.154s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  3  y con   1  hilos, intento numero 1\n",
            "real 0m0.646s user 0m0.460s sys 0m0.170s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  5  y con   1  hilos, intento numero 1\n",
            "real 0m0.639s user 0m0.447s sys 0m0.170s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  7  y con   1  hilos, intento numero 1\n",
            "real 0m0.678s user 0m0.472s sys 0m0.193s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  9  y con   1  hilos, intento numero 1\n",
            "real 0m0.718s user 0m0.485s sys 0m0.224s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  11  y con   1  hilos, intento numero 1\n",
            "real 0m0.745s user 0m0.499s sys 0m0.233s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  13  y con   1  hilos, intento numero 1\n",
            "real 0m0.762s user 0m0.497s sys 0m0.253s\n",
            "Tiempo para imagen /content/src/1080.jpg  con un kernel de  15  y con   1  hilos, intento numero 1\n",
            "real 0m0.792s user 0m0.522s sys 0m0.252s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  3  y con   1  hilos, intento numero 1\n",
            "real 0m2.145s user 0m1.853s sys 0m0.277s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  5  y con   1  hilos, intento numero 1\n",
            "real 0m2.258s user 0m1.923s sys 0m0.324s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  7  y con   1  hilos, intento numero 1\n",
            "real 0m2.530s user 0m2.052s sys 0m0.458s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  9  y con   1  hilos, intento numero 1\n",
            "real 0m2.809s user 0m2.211s sys 0m0.585s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  11  y con   1  hilos, intento numero 1\n",
            "real 0m3.048s user 0m2.383s sys 0m0.655s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  13  y con   1  hilos, intento numero 1\n",
            "real 0m3.212s user 0m2.421s sys 0m0.768s\n",
            "Tiempo para imagen /content/src/4k.jpg  con un kernel de  15  y con   1  hilos, intento numero 1\n",
            "real 0m3.342s user 0m2.533s sys 0m0.790s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnY3sSfSs_iA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}